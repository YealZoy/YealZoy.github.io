[{"title":"istio metrics(六)","date":"2018-07-17T13:54:48.000Z","path":"201807/17/istio-metrics/","text":"Istio MetricsService Telemetry# Configuration for metric instancesapiVersion: \"config.istio.io/v1alpha2\"kind: metricmetadata: name: doublerequestcount namespace: istio-systemspec: value: \"2\" # count each request twice dimensions: source: source.service | \"unknown\" destination: destination.service | \"unknown\" message: '\"twice the fun!\"' monitored_resource_type: '\"UNSPECIFIED\"'---# Configuration for a Prometheus handlerapiVersion: \"config.istio.io/v1alpha2\"kind: prometheusmetadata: name: doublehandler namespace: istio-systemspec: metrics: - name: double_request_count # Prometheus metric name instance_name: doublerequestcount.metric.istio-system # Mixer instance name (fully-qualified) kind: COUNTER label_names: - source - destination - message---# Rule to send metric instances to a Prometheus handlerapiVersion: \"config.istio.io/v1alpha2\"kind: rulemetadata: name: doubleprom namespace: istio-systemspec: actions: - handler: doublehandler.prometheus instances: - doublerequestcount.metric---# Configuration for logentry instancesapiVersion: \"config.istio.io/v1alpha2\"kind: logentrymetadata: name: newlog namespace: istio-systemspec: severity: '\"warning\"' timestamp: request.time variables: source: source.labels[\"app\"] | source.service | \"unknown\" user: source.user | \"unknown\" destination: destination.labels[\"app\"] | destination.service | \"unknown\" responseCode: response.code | 0 responseSize: response.size | 0 latency: response.duration | \"0ms\" monitored_resource_type: '\"UNSPECIFIED\"'---# Configuration for a stdio handlerapiVersion: \"config.istio.io/v1alpha2\"kind: stdiometadata: name: newhandler namespace: istio-systemspec: severity_levels: warning: 1 # Params.Level.WARNING outputAsJson: true---# Rule to send logentry instances to a stdio handlerapiVersion: \"config.istio.io/v1alpha2\"kind: rulemetadata: name: newlogstdio namespace: istio-systemspec: match: \"true\" # match for all requests actions: - handler: newhandler.stdio instances: - newlog.logentry--- TCP Telemetry# Configuration for a metric measuring bytes sent from a server# to a clientapiVersion: \"config.istio.io/v1alpha2\"kind: metricmetadata: name: mongosentbytes namespace: defaultspec: value: connection.sent.bytes | 0 # uses a TCP-specific attribute dimensions: source_service: source.service | \"unknown\" source_version: source.labels[\"version\"] | \"unknown\" destination_version: destination.labels[\"version\"] | \"unknown\" monitoredResourceType: '\"UNSPECIFIED\"'---# Configuration for a metric measuring bytes sent from a client# to a serverapiVersion: \"config.istio.io/v1alpha2\"kind: metricmetadata: name: mongoreceivedbytes namespace: defaultspec: value: connection.received.bytes | 0 # uses a TCP-specific attribute dimensions: source_service: source.service | \"unknown\" source_version: source.labels[\"version\"] | \"unknown\" destination_version: destination.labels[\"version\"] | \"unknown\" monitoredResourceType: '\"UNSPECIFIED\"'---# Configuration for a Prometheus handlerapiVersion: \"config.istio.io/v1alpha2\"kind: prometheusmetadata: name: mongohandler namespace: defaultspec: metrics: - name: mongo_sent_bytes # Prometheus metric name instance_name: mongosentbytes.metric.default # Mixer instance name (fully-qualified) kind: COUNTER label_names: - source_service - source_version - destination_version - name: mongo_received_bytes # Prometheus metric name instance_name: mongoreceivedbytes.metric.default # Mixer instance name (fully-qualified) kind: COUNTER label_names: - source_service - source_version - destination_version---# Rule to send metric instances to a Prometheus handlerapiVersion: \"config.istio.io/v1alpha2\"kind: rulemetadata: name: mongoprom namespace: defaultspec: match: context.protocol == \"tcp\" &amp;&amp; destination.service == \"mongodb.default.svc.cluster.local\" actions: - handler: mongohandler.prometheus instances: - mongoreceivedbytes.metric - mongosentbytes.metric 参考文档istio metrics","tags":[{"name":"service Mesh","slug":"service-Mesh","permalink":"https://www.zhengyuyan.com/tags/service-Mesh/"}]},{"title":"istio策略管理(五)","date":"2018-07-17T13:51:44.000Z","path":"201807/17/istio-policy/","text":"Istio 策略管理Mixer 为应用程序和基础架构后端之间提供了一个通用的策略控制层，负责先决条件检查（如认证授权）、配额管理并从 Envoy 代理中收集遥测数据等。 Mixer 支持灵活的插件模型（即 Adapters），支持 GCP、AWS、Prometheus、Heapster 等各种丰富功能的后端。 实现原理本质上，Mixer 是一个 属性 处理机，进入 Mixer 的请求带有一系列的属性，Mixer 按照不同的处理阶段处理： 通过全局 Adapters 为请求引入新的属性 通过解析（Resolution）识别要用于处理请求的配置资源 处理属性，生成 Adapter 参数 分发请求到各个 Adapters 后端处理 Adapters 后端以 Mixer 配置 的方式注册到 Istio 中，参考这里查看示例配置 流量限制示例apiVersion: \"config.istio.io/v1alpha2\"kind: memquotametadata: name: handler namespace: istio-systemspec: quotas: - name: requestcount.quota.istio-system maxAmount: 5000 validDuration: 1s # The first matching override is applied. # A requestcount instance is checked against override dimensions. overrides: # The following override applies to 'ratings' when # the source is 'reviews'. - dimensions: destination: ratings source: reviews maxAmount: 1 validDuration: 1s # The following override applies to 'ratings' regardless # of the source. - dimensions: destination: ratings maxAmount: 100 validDuration: 1s---apiVersion: \"config.istio.io/v1alpha2\"kind: quotametadata: name: requestcount namespace: istio-systemspec: dimensions: source: source.labels[\"app\"] | source.service | \"unknown\" sourceVersion: source.labels[\"version\"] | \"unknown\" destination: destination.labels[\"app\"] | destination.service | \"unknown\" destinationVersion: destination.labels[\"version\"] | \"unknown\"---apiVersion: \"config.istio.io/v1alpha2\"kind: rulemetadata: name: quota namespace: istio-systemspec: actions: - handler: handler.memquota instances: - requestcount.quota---apiVersion: config.istio.io/v1alpha2kind: QuotaSpecmetadata: creationTimestamp: null name: request-count namespace: istio-systemspec: rules: - quotas: - charge: 1 quota: RequestCount---apiVersion: config.istio.io/v1alpha2kind: QuotaSpecBindingmetadata: creationTimestamp: null name: request-count namespace: istio-systemspec: quotaSpecs: - name: request-count namespace: istio-system services: - name: ratings - name: reviews - name: details - name: productpage 参考文档istio策略管理","tags":[{"name":"service Mesh","slug":"service-Mesh","permalink":"https://www.zhengyuyan.com/tags/service-Mesh/"}]},{"title":"istio安全管理(四)","date":"2018-07-17T13:48:36.000Z","path":"201807/17/istio-aq/","text":"Istio 安全管理Istio 提供了 RBAC 访问控制以及双向 TLS 认证等安全管理功能。 RBACIstio Role-Based Access Control (RBAC) 提供了 namespace、service 以及 method 级别的访问控制。其特性包括 简单易用：提供基于角色的语意 支持认证：提供服务 - 服务和用户 - 服务的认证 灵活：提供角色和角色绑定的自定义属性 开启RBAC# Enable RBAC for default namespaceistioctl create -f samples/bookinfo/kube/istio-rbac-enable.yaml 实现原理在实现原理上，Istio RBAC 作为 Mixer Adaper 对请求上下文（Request Context）进行认证，并返回授权结果：ALLOW 或者 DENY。请求上下文包含访问对象和动作等两部分，如apiVersion: \"config.istio.io/v1alpha2\"kind: authorizationmetadata: name: requestcontext namespace: istio-systemspec: subject: user: source.user | \"\" groups: \"\" properties: app: source.labels[\"app\"] | \"\" version: source.labels[\"version\"] | \"\" namespace: source.namespace | \"\" action: namespace: destination.namespace | \"\" service: destination.service | \"\" method: request.method | \"\" path: request.path | \"\" properties: app: destination.labels[\"app\"] | \"\" version: destination.labels[\"version\"] | \"\"---apiVersion: \"config.istio.io/v1alpha2\"kind: rbacmetadata: name: handler namespace: istio-systemspec: config_store_url: \"k8s://\"---apiVersion: \"config.istio.io/v1alpha2\"kind: rulemetadata: name: rbaccheck namespace: istio-systemspec: match: destination.namespace == \"default\" actions: - handler: handler.rbac instances: - requestcontext.authorization 访问控制Istio RBAC 提供了 ServiceRole 和 ServiceRoleBinding 两种资源对象，并以 CustomResourceDefinition (CRD) 的方式管理。 ServiceRole 定义了一个可访问特定资源（namespace 之内）的服务角色，并支持以前缀通配符和后缀通配符的形式匹配一组服务 ServiceRoleBinding 定义了赋予指定角色的绑定，即可以指定的角色和动作访问服务apiVersion: \"config.istio.io/v1alpha2\"kind: ServiceRolemetadata: name: service-viewer namespace: defaultspec: rules: - services: [\"*\"] methods: [\"GET\"] constraints: - key: \"app\" values: [\"productpage\", \"details\", \"reviews\", \"ratings\"]---apiVersion: \"config.istio.io/v1alpha2\"kind: ServiceRoleBindingmetadata: name: bind-service-viewer namespace: defaultspec: subjects: - properties: namespace: \"default\" - properties: namespace: \"istio-system\" roleRef: kind: ServiceRole name: \"service-viewer\" 双向 TLS双向 TLS 为服务间通信提供了 TLS 认证，并提供管理系统自动管理密钥和证书的生成、分发、替换以及撤销。 实现原理Istio Auth 由三部分组成： 身份（Identity）：Istio 使用 Kubernetes service account 来识别服务的身份，格式为 spiffe://&lt;domain&gt;/ns/&lt;namespace&gt;/sa/&lt;serviceaccount&gt; 通信安全：端到端 TLS 通信通过服务器端和客户端的 Envoy 容器完成 证书管理：Istio CA (Certificate Authority) 负责为每个 service account 生成 SPIFEE 密钥和证书、分发到 Pod（通过 Secret Volume Mount 的形式）、定期轮转（Rotate）以及必要时撤销。对于 Kuberentes 之外的服务，CA 配合 Istio node agent 共同完成整个过程。 这样，一个容器使用证书的流程为 首先，Istio CA 监听 Kubernetes API，并为 service account 生成 SPIFFE 密钥及证书，再以 secret 形式存储到 Kubernetes 中 然后，Pod 创建时，Kubernetes API Server 将 secret 挂载到容器中 最后，Pilot 生成一个访问控制的配置，定义哪些 service account 可以访问服务，并分发给 Envoy 而当容器间通信时，Pod 双方的 Envoy 就会基于访问控制配置来作认证 最佳实践 为不同团队创建不同 namespace 分别管理 将 Istio CA 运行在单独的 namespace 中，并且仅授予管理员权限 参考文档istio安全管理","tags":[{"name":"service Mesh","slug":"service-Mesh","permalink":"https://www.zhengyuyan.com/tags/service-Mesh/"}]},{"title":"istio流量管理(三)","date":"2018-07-17T13:43:40.000Z","path":"201807/17/istio-traffic/","text":"Istio 流量管理Istio 提供了强大的流量管理功能，如智能路由、服务发现与负载均衡、故障恢复、故障注入等。 流量管理的功能由 Pilot 配合 Envoy 负责，并接管进入和离开容器的所有流量： API 版本Istio 0.7.X 及以前版本仅支持 config.istio.io/v1alpha2，0.8.0 将其升级为 networking.istio.io/v1alpha3，并且重命名了流量管理的几个资源对象： RouteRule -&gt; VirtualService DestinationPolicy -&gt; DestinationRule EgressRule -&gt; ExternalService Ingress -&gt; Gateway 服务发现和负载均衡为了接管流量，Istio 假设所有容器在启动时自动将自己注册到 Istio 中（通过自动或手动给 Pod 注入 Envoy sidecar 容器）。Envoy 收到外部请求后，会对请求作负载均衡，并支持轮询、随机和加权最少请求等负载均衡算法。除此之外，Envoy 还会以熔断机制定期检查服务后端容器的健康状态，自动移除不健康的容器和加回恢复正常的容器。容器内也可以返回 HTTP 503 显示将自己从负载均衡中移除。 流量接管原理Envoy sidecar 使用 iptables 把进入 Pod 和从 Pod 发出的流量转发到 Envoy 进程监听的端口（即 15001 端口）上：# Generated by iptables-save v1.6.0 on Fri Jun 8 07:37:33 2018*mangle:PREROUTING ACCEPT [2883:1129969]:INPUT ACCEPT [2883:1129969]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [58217:6662600]:POSTROUTING ACCEPT [58217:6662600]COMMIT# Completed on Fri Jun 8 07:37:33 2018# Generated by iptables-save v1.6.0 on Fri Jun 8 07:37:33 2018*nat:PREROUTING ACCEPT [0:0]:INPUT ACCEPT [0:0]:OUTPUT ACCEPT [299:27815]:POSTROUTING ACCEPT [299:27815]:ISTIO_INBOUND - [0:0]:ISTIO_OUTPUT - [0:0]:ISTIO_REDIRECT - [0:0]-A PREROUTING -p tcp -j ISTIO_INBOUND-A OUTPUT -p tcp -j ISTIO_OUTPUT-A ISTIO_INBOUND -p tcp -m tcp --dport 9080 -j ISTIO_REDIRECT-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -j ISTIO_REDIRECT-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN-A ISTIO_OUTPUT -j ISTIO_REDIRECT-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001COMMIT# Completed on Fri Jun 8 07:37:33 2018 故障恢复Istio 提供了一系列开箱即用的故障恢复功能，如 超时处理 重试处理，如限制最大重试时间以及可变重试间隔 健康检查，如自动移除不健康的容器 请求限制，如并发请求数和并发连接数 熔断 这些功能均可以使用 VirtualService 动态配置。比如以下为用户 jason 的请求返回 500 （而其他用户均可正常访问）：apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: ratingsspec: hosts: - ratings http: - match: - headers: cookie: regex: \"^(.*?;)?(user=jason)(;.*)?$\" fault: abort: percent: 100 httpStatus: 500 route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1 熔断示例：apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: httpbinspec: host: httpbin trafficPolicy: connectionPool: tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: http: consecutiveErrors: 1 interval: 1s baseEjectionTime: 3m maxEjectionPercent: 100 故障注入Istio 支持为应用注入故障，以模拟实际生产中碰到的各种问题，包括 注入延迟（模拟网络延迟和服务过载） 注入失败（模拟应用失效）这些故障均可以使用 VirtualService 动态配置。如以下配置 2 秒的延迟：apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: ratingsspec: hosts: - ratings http: - fault: delay: percent: 100 fixedDelay: 2s route: - destination: host: ratings subset: v1 金丝雀部署 首先部署 bookinfo，并配置默认路由为 v1 版本：apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: productpagespec: hosts: - productpage gateways: - bookinfo http: - route: - destination: host: productpage subset: v1---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: ratingsspec: hosts: - ratings http: - route: - destination: host: ratings subset: v1---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: detailsspec: hosts: - details http: - route: - destination: host: details subset: v1---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: productpagespec: host: productpage subsets: - name: v1 labels: version: v1------apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: reviewsspec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: ratingsspec: host: ratings subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v2-mysql labels: version: v2-mysql - name: v2-mysql-vm labels: version: v2-mysql-vm---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: detailsspec: host: details subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2--- 示例一：将 10% 请求发送到 v2 版本而其余 90% 发送到 v1 版本apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 90 - destination: host: reviews subset: v2 weight: 10 示例二：将 jason 用户的请求全部发到 v2 版本apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - match: - headers: cookie: regex: \"^(.*?;)?(user=jason)(;.*)?$\" route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1 示例三：全部切换到 v2 版本apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: reviewsspec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 示例四：限制并发访问apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: reviewsspec: host: reviews subsets: - name: v1 labels: version: v1 trafficPolicy: connectionPool: tcp: maxConnections: 100 Istio IngressIstio 在部署时会自动创建一个 Istio Gateway，用来控制 Ingress 访问。# prepare自动注入kubectl apply -f samples/httpbin/httpbin.yaml# get ingress external IP (suppose load balancer service)kubectl get svc istio-ingressgateway -n istio-systemexport INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.status.loadBalancer.ingress[0].ip&#125;')export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name==\"http\")].port&#125;')export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name==\"https\")].port&#125;')# create gatewaycat &lt;&lt;EOF | istioctl create -f -apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: httpbin-gatewayspec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"httpbin.example.com\"EOF# configure routes for the gatewaycat &lt;&lt;EOF | istioctl create -f -apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - \"httpbin.example.com\" gateways: - httpbin-gateway http: - match: - uri: prefix: /status - uri: prefix: /delay route: - destination: port: number: 8000 host: httpbinEOF# validate 200curl --resolve httpbin.example.com:$INGRESS_PORT:$INGRESS_HOST -HHost:httpbin.example.com -I http://httpbin.example.com:$INGRESS_PORT/status/200# invalidate 404curl --resolve httpbin.example.com:$INGRESS_PORT:$INGRESS_HOST -HHost:httpbin.example.com -I http://httpbin.example.com:$INGRESS_PORT/headers 使用 TLS：# 生成证书git clone https://github.com/nicholasjackson/mtls-go-examplecd mtls-go-example#该命令将产生四个目录：1_root，2_intermediate，3_application和4_client与你将使用客户端和服务器证书。generate.sh bookinfo.example.com &lt;password&gt;# istio-ingressgateway-certs必须在istio-system命名空间中kubectl create -n istio-system secret tls istio-ingressgateway-certs --key 3_application/private/httpbin.example.com.key.pem --cert 3_application/certs/httpbin.example.com.cert.pemcat &lt;&lt;EOF | istioctl replace -f -apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: httpbin-gatewayspec: selector: istio: ingressgateway # use istio default ingress gateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"httpbin.example.com\" - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE serverCertificate: /etc/istio/ingressgateway-certs/tls.crt privateKey: /etc/istio/ingressgateway-certs/tls.key hosts: - \"httpbin.example.com\"EOF# validate 200curl --resolve httpbin.example.com:$SECURE_INGRESS_PORT:$INGRESS_HOST -HHost:httpbin.example.com -I -k https://httpbin.example.com:$SECURE_INGRESS_PORT/status/200 Egress 流量默认情况下，Istio 接管了容器的内外网流量，从容器内部无法访问 Kubernetes 集群外的服务。可以通过 ServiceEntry 为需要的容器开放 Egress 访问，如$ cat &lt;&lt;EOF | istioctl create -f -apiVersion: networking.istio.io/v1alpha3kind: ServiceEntrymetadata: name: httpbin-extspec: hosts: - httpbin.org ports: - number: 80 name: http protocol: HTTPEOF$ cat &lt;&lt;EOF | istioctl create -f -apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbin-extspec: hosts: - httpbin.org http: - timeout: 3s route: - destination: host: httpbin.org weight: 100EOF 需要注意的是 ServiceEntry 仅支持 HTTP、TCP 和 HTTPS，对于其他协议需要通过 –includeIPRanges 的方式设置 IP 地址范围，如helm template @install/kubernetes/helm/istio@ --name istio --namespace istio-system --set global.proxy.includeIPRanges=\"10.0.0.1/24\" -x @templates/sidecar-injector-configmap.yaml@ | kubectl apply -f - 流量镜像cat &lt;&lt;EOF | istioctl replace -f -apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: httpbinspec: hosts: - httpbin http: - route: - destination: host: httpbin subset: v1 weight: 100 mirror: host: httpbin subset: v2EOF 参考文档istio官方文档istio流量管理","tags":[{"name":"service Mesh","slug":"service-Mesh","permalink":"https://www.zhengyuyan.com/tags/service-Mesh/"}]},{"title":"istio官方示例bookinfo","date":"2018-07-17T13:38:19.000Z","path":"201807/17/bookinfo/","text":"官方bookinfo示例bookinfo显示有关书籍的信息，类似于在线书店的单个商品。页面上显示的是书籍，书籍详细信息（ISBN，页数等）以及一些书评。 Bookinfo应用程序分为四个独立的微服务： productpage。productpage微服务调用详细信息并查看微服务以填充页面。 details。details微服务包含书籍信息。 reviews。reviews微服务包含书评。它还称为评级微服务。 ratings。ratings微服务包含伴随书评的书籍排名信息。 reviews微服务有3个版本： 版本v1不会调用评级服务。 版本v2调用评级服务，并将每个评级显示为1到5个黑色星。 版本v3调用评级服务，并将每个评级显示为1到5个红星。 应用程序的端到端架构如下所示。 部署应用程序使用Istio不需要更改应用程序本身。相反，我们只需要在启用Istio的环境中配置和运行服务，并在每个服务旁边注入Envoy边车。所需的命令和配置因运行时环境而异，但在所有情况下，生成的部署将如下所示： 创建istio-app命名空间kubectl create namespace istio-app 将istio-app命名空间设置为自动注入标签kubectl label namespace istio-app istio-injection=enabled 执行kubectl apply -f samples/bookinfo/kube/bookinfo.yaml 部署好的productpage会自动注入sideCar 为程序定义网关入口istioctl create -f gateway/gateway.yaml -n istio-app apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: istio-gatewayspec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \"bookinfo.example.com\" 创建VirtualService apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: bookinfospec: hosts: - \"bookinfo.example.com\" gateways: - istio-gateway http: - match: - uri: exact: /productpage - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage.istio-app.svc.cluster.local port: number: 9080 参考文档isito官方示例","tags":[{"name":"service Mesh","slug":"service-Mesh","permalink":"https://www.zhengyuyan.com/tags/service-Mesh/"}]},{"title":"istio 安装(二)","date":"2018-07-17T13:28:42.000Z","path":"201807/17/istio-install/","text":"istio安装istio官网安装提供了多种方法，因为集群中包含helm，所以选择用helm安装istio 下载istio release(目前版本为0.8.0) 命令行执行helm install install/kubernetes/helm/istio --name istio --namespace istio-system(此命令采取自动注入sideCar) istio的helm的value.yaml文件如下:# Use --set or additional values.yaml file to configure settings.# This file no longer uses sed, updateVersions.sh or istio.VERSIONS# TODO: evaluate if we need individual overrides for each component version, istio# is not typically tested with a mix of versions. Only supported case is version upgrade.# Common settings.global: # Default repository for Istio images. # Releases are published to docker hub under 'istio' project. # Daily builds from prow are on gcr.io, and nightly builds from circle on # docker.io/istionightly hub: 192.168.30.100:8889/istio # Default tag for Istio images. # Should track latest released version in the branch. tag: 0.8.0 proxy: image: proxyv2 resources: requests: cpu: 100m memory: 128Mi # istio-sidecar-injector configmap stores configuration for sidecar injection. # This config map is used by istioctl kube-inject and the injector webhook. enableCoreDump: false serviceAccountName: default # used only if RBAC is not enabled replicaCount: 1 resources: requests: cpu: 100m memory: 128Mi # limits: # cpu: 100m # memory: 128Mi # istio egress capture whitelist # https://istio.io/docs/tasks/traffic-management/egress.html#calling-external-services-directly # example: includeIPRanges: \"172.30.0.0/16,172.20.0.0/16\" # would only capture egress traffic on those two IP Ranges, all other outbound traffic would # be allowed by the sidecar includeIPRanges: \"*\" excludeIPRanges: \"\" # istio ingress capture whitelist # examples: # Redirect no inbound traffic to Envoy: --includeInboundPorts=\"\" # Redirect all inbound traffic to Envoy: --includeInboundPorts=\"*\" # Redirect only selected ports: --includeInboundPorts=\"80,8080\" includeInboundPorts: \"*\" excludeInboundPorts: \"\" policy: enabled proxy_init: image: proxy_init # imagePullPolicy is applied to istio control plane components. # local tests require IfNotPresent, to avoid uploading to dockerhub. # TODO: Switch to Always as default, and override in the local tests. imagePullPolicy: IfNotPresent # Not recommended for user to configure this. Hyperkube image to use when creating custom resources hyperkube: repository: 192.168.30.100:8889/coreos/hyperkube tag: v1.7.6_coreos.0 # controlPlaneMtls enabled. Will result in delays starting the pods while secrets are # propagated, not recommended for tests. controlPlaneSecurityEnabled: false # Default mtls policy. If true, mtls between services will be enabled by default. mtls: # Default setting for service-to-service mtls. Can be set explicitly using # destination rules or service annotations. enabled: false # List of fully qualified services to exclude from mtls # TODO: add the templating. mtlsExcludedServices: - \"kubernetes.default.svc.cluster.local\" # create RBAC resources. Must be set for any cluster configured with rbac. rbacEnabled: true ## imagePullSecrets for all ServiceAccount. Must be set for any clustser configured with privte docker registry. # imagePullSecrets: # - name: \"private-registry-key\" # Default is 1 second refreshInterval: 10s # Enable multicluster operation. Must be set to true if multicluster operation # is desired. multicluster: enabled: false # Specify pod scheduling arch(amd64, ppc64le, s390x) and weight as follows: # 0 - Never scheduled # 1 - Least preferred # 2 - No preference # 3 - Most preferred arch: amd64: 2 s390x: 2 ppc64le: 2# Any customization for istio testing should be hereistiotesting: oneNameSpace: false## ingress configuration#ingress: enabled: true serviceAccountName: default autoscaleMin: 1 autoscaleMax: 1 resources: &#123;&#125;# limits:# cpu: 100m# memory: 128Mi# requests:# cpu: 100m# memory: 128Mi service: loadBalancerIP: \"10.100.100.20\" type: LoadBalancer #change to NodePort, ClusterIP or LoadBalancer if need be ports: - port: 80 name: http nodePort: 32000 - port: 443 name: https selector: istio: ingress## ingressgateway configuration#ingressgateway: enabled: true serviceAccountName: istio-ingressgateway-service-account autoscaleMin: 1 autoscaleMax: 1 resources: &#123;&#125;# limits:# cpu: 100m# memory: 128Mi# requests:# cpu: 100m# memory: 128Mi service: name: istio-ingressgateway #DNS addressible labels: istio: ingressgateway #namespace: istio-system loadBalancerIP: \"10.100.100.129\" type: LoadBalancer #change to NodePort, ClusterIP or LoadBalancer if need be ports: ## You can add custom gateway ports - port: 80 name: http nodePort: 31380 - port: 443 name: https nodePort: 31390 - port: 31400 name: tcp nodePort: 31400 deployment: labels: istio: ingressgateway #will be added to pods and service ports: - containerPort: 80 - containerPort: 443 - containerPort: 31400 secretVolumes: - name: ingressgateway-certs secretName: istio-ingressgateway-certs mountPath: /etc/istio/ingressgateway-certs## egressgateway configuration#egressgateway: enabled: true serviceAccountName: istio-egressgateway-service-account autoscaleMin: 1 autoscaleMax: 1 resources: &#123;&#125;# limits:# cpu: 100m# memory: 128Mi# requests:# cpu: 100m# memory: 128Mi service: name: istio-egressgateway #DNS addressible labels: istio: egressgateway #namespace: istio-system type: ClusterIP #change to NodePort or LoadBalancer if need be ports: ## You can add custom gateway ports - port: 80 name: http - port: 443 name: https deployment: labels: istio: egressgateway #will be added to pods and service ports: - containerPort: 80 - containerPort: 443 # secretVolumes: TODO # - name: someName # mountPath: somePath # secretName: someName## sidecar-injector webhook configuration#sidecarInjectorWebhook: enabled: true image: sidecar_injector## galley configuration#galley: enabled: false serviceAccountName: default replicaCount: 1 image: galley resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi## mixer configuration#mixer: enabled: true serviceAccountName: default # used only if RBAC is not enabled replicaCount: 1 image: mixer resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi prometheusStatsdExporter: repository: 192.168.30.100:8889/prom/statsd-exporter tag: latest resources: &#123;&#125;## pilot configuration#pilot: enabled: true serviceAccountName: default # used only if RBAC is not enabled replicaCount: 1 image: pilot resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi## security configuration#security: enabled: true serviceAccountName: default # used only if RBAC is not enabled replicaCount: 1 image: citadel resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi cleanUpOldCA: true## addons configuration#grafana: enabled: true replicaCount: 1 image: grafana service: name: http type: ClusterIP externalPort: 3000 internalPort: 3000 ingress: enabled: false # Used to create an Ingress record. hosts: - grafana.local annotations: # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" tls: # Secrets must be manually created in the namespace. # - secretName: grafana-tls # hosts: # - grafana.local resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Miprometheus: enabled: true replicaCount: 1 image: repository: 192.168.30.100:8889/prom/prometheus tag: latest ingress: enabled: false # Used to create an Ingress record. #hosts: # - prometheus.local annotations: # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" tls: # Secrets must be manually created in the namespace. # - secretName: prometheus-tls # hosts: # - prometheus.local resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi service: nodePort: enabled: false port: 32090servicegraph: enabled: true replicaCount: 1 image: servicegraph service: name: http type: ClusterIP externalPort: 8088 internalPort: 8088 ingress: enabled: false # Used to create an Ingress record. hosts: - servicegraph.local annotations: # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" tls: # Secrets must be manually created in the namespace. # - secretName: servicegraph-tls # hosts: # - servicegraph.local resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi # prometheus addres prometheusAddr: http://prometheus:9090tracing: enabled: true jaeger: enabled: true memory: max_traces: 50000 replicaCount: 1 image: repository: 192.168.30.100:8889/jaegertracing/all-in-one tag: 1.5 service: name: http type: ClusterIP externalPort: 9411 internalPort: 9411 uiPort: 16686 ingress: enabled: false # Used to create an Ingress record. hosts: - zipkin.local annotations: # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" tls: # Secrets must be manually created in the namespace. # - secretName: zipkin-tls # hosts: # - zipkin.local resources: &#123;&#125; # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi 主要修改了镜像地址和将以些默认不安装的addon设置为安装(grafana,servicegraph,tracing)安装成功之后 istio svc 为了避免ingressgate-way所在的机子80端口被占用，特申请一台ingressgate-way的IP地址相同机子在dns里添加了主机域名 Mesh ExpansionIstio 还支持管理非 Kubernetes 管理的应用。此时，需要在应用所在的 VM 或者物理中部署 Istio，具体步骤请参考 https://istio.io/docs/setup/kubernetes/mesh-expansion.html。部署好后，就可以向 Istio 注册应用，如# istioctl register servicename machine-ip portname:portistioctl -n onprem register mysql 1.2.3.4 3306istioctl -n onprem register svc1 1.2.3.4 http:7000 Prometheus、Grafana 和 Zipkin等所有 Pod 启动后，可以通过 NodePort、负载均衡服务的外网 IP 或者 kubectl proxy 来访问这些服务或者创建一个istio的VirtualService。apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: istio-gatewayspec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \"prom.example.com\" - \"grafana.example.com\" - \"pilot.example.com\" - \"zipkin.example.com\"---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: grafanaspec: hosts: - \"grafana.example.com\" gateways: - istio-gateway http: - route: - destination: host: grafana.istio-system.svc.cluster.local port: number: 3000---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: promspec: hosts: - \"prom.example.com\" gateways: - istio-gateway http: - route: - destination: host: prometheus.istio-system.svc.cluster.local port: number: 9090--- 通过 http://grafana.example.com 访问 Grafana 服务 通过loabblancer访问jaeger-query,展示服务之间调用关系图 参考文档官方istio安装istio安装","tags":[{"name":"service Mesh","slug":"service-Mesh","permalink":"https://www.zhengyuyan.com/tags/service-Mesh/"}]},{"title":"istio简介(一)","date":"2018-07-17T13:17:51.000Z","path":"201807/17/istio-brief/","text":"Istio简介Service Mesh服务网格是一个基础设施层，功能在于处理服务间通信，职责是负责实现请求的可靠传递。在实践中，服务网格通常实现为轻量级网络代理，通常与应用程序部署在一起，但是对应用程序透明。 Istio是什么Istio是一个服务网格.Istio 是一个用来连接、管理和保护微服务的开放平台。Istio 提供一种简单的方式来为已部署的服务建立网络，该网络具有负载均衡、服务间认证、监控等功能，而不需要对服务的代码做任何改动。想要让服务支持 Istio，只需要在您的环境中部署一个特殊的 sidecar，使用 Istio 控制平面功能配置和管理代理，拦截微服务之间的所有网络通信。 Istio功能Istio 提供了一个完整的解决方案，通过为整个服务网格提供行为洞察和操作控制来满足微服务应用程序的多样化需求。它在服务网络中统一提供了许多关键功能： 流量管理。控制服务之间的流量和API调用的流向，使得调用更可靠，并使网络在恶劣情况下更加健壮。 服务身份和安全。为网格中的服务提供可验证身份，并提供保护服务流量的能力，使其可以在不同可信度的网络上流转。 策略执行。将组织策略应用于服务之间的互动，确保访问策略得以执行，资源在消费者之间良好分配。可以通过通过配置网格而不是修改应用程序代码来完成策略的更改。 遥测：了解服务之间的依赖关系，以及它们之间流量的本质和流向，从而提供快速识别问题的能力。 Istio体系结构Istio 服务网格逻辑上分为数据平面和控制平面。 数据平面由一组以sidecar方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及 Mixer 之间所有的网络通信。 控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。 Istio主要由Envoy、Pilot、Mixer三部分组成，整体结构如下： Envoy：以sidecar的形式和应用程序运行与同一个pod中，通过修改iptables来代理应用程序的所有出入流量 Pilot：接受系统管理员发出的指令或者规则，遥控所有Envoy的行为 Mixer：从Envoy处获取流量属性，根据自定义的属性匹配规则进行流量处理，如：流量限制、日志记录等 EnvoyIstio利用了Envoy的许多内置功能，例如动态服务发现，负载均衡，TLS termination，HTTP/2&amp;gRPC代理，熔断器，健康检查，基于百分比流量拆分的分段推出，故障注入和丰富的metrics。Envoy实现了过滤和路由、服务发现、健康检查，提供了具有弹性的负载均衡。它在安全上支持TLS，在通信方面支持gRPC. 概括说，Envoy 提供的是服务间网络通讯的能力，包括(以下均可支持TLS)： HTTP／1.1 HTTP/2 gRPC TCP 以及网络通讯直接相关的功能： 服务发现：从Pilot得到服务发现信息 过滤 负载均衡 健康检查 执行路由规则(Rule): 规则来自Polit,包括路由和目的地策略 加密和认证: TLS certs来自 istio-Auth Pilot(流量管理)Envoy在其中扮演的负责搬砖的民工角色, 而指挥Envoy工作的民工头就是Pilot模块.Pilot负责收集和验证配置并将其传播到各种Istio组件。它从Mixer和Envoy中抽取环境特定的实现细节，为他们提供独立于底层平台的用户服务的抽象表示。此外，流量管理规则（即通用4层规则和7层HTTP/gRPC路由规则）可以在运行时通过Pilot进行编程。每个Envoy实例根据其从Pilot获得的信息以及其负载均衡池中的其他实例的定期健康检查来维护 负载均衡信息，从而允许其在目标实例之间智能分配流量，同时遵循其指定的路由规则。Pilot负责在Istio服务网格中部署的Envoy实例的生命周期。 Pilot的架构 Envoy API负责和Envoy的通讯, 主要是发送服务发现信息和流量控制规则给Envoy Envoy提供服务发现，负载均衡池和路由表的动态更新的API。这些API将istio和Envoy的实现解耦。(另外,也使得 Linkerd 之类的其他服务网络实现得以平滑接管Envoy) Polit 定了一个抽象模型, 以从特定平台细节中解耦, 为跨平台提供基础. Platform Adapter则是这个抽象模型的现实实现版本, 用于对接外部的不同平台 最后是 Rules API, 提供接口给外部调用以管理 Pilot, 包括命令行工具istioctl以及未来可能出现的第三方管理界面 pilot功能基于上述的架构设计, pilot提供以下重要功能: 请求路由 服务发现和负载均衡 故障处理 故障注入 规则配置 Mixer Mixer负责在服务网格上执行访问控制和使用策略，并收集Envoy代理和其他服务的遥测数据。 Mixer的设计背景我们的系统通常会基于大量的基础设施而构建, 这些基础设施的后端服务为业务服务提供各种支持功能。包括访问控制系统，遥测捕获系统，配额执行系统，计费系统等。在传统设计中, 服务直接与这些后端系统集成，容易产生硬耦合.在istio中,为了避免应用程序的微服务和基础设施的后端服务之间的耦合, 提供了 Mixer 作为两者的通用中介层: Mixer的功能Mixer 提供三个核心功能： 前提条件检查。允许服务在响应来自服务消费者的传入请求之前验证一些前提条件。前提条件包括认证，黑白名单，ACL检查等等。 配额管理。使服务能够在多个维度上分配和释放配额。典型例子如限速。 遥测报告。使服务能够上报日志和监控。在Istio内，Envoy重度依赖Mixer。 Istio-AuthIstio-Auth提供强大的服务到服务和终端用户认证，使用交互TLS，内置身份和凭据管理。它可用于升级服务网格中的未加密流量，并为运维人员提供基于服务身份而不是网络控制实施策略的能力。 auth的架构其中包括三个组件：身份，密钥管理和通信安全。 参考文档istio官网服务网格新生代-Istio(敖小剑)istio简介","tags":[{"name":"service Mesh","slug":"service-Mesh","permalink":"https://www.zhengyuyan.com/tags/service-Mesh/"}]},{"title":"etcd操作","date":"2018-06-29T09:22:39.000Z","path":"201806/29/etcd操作/","text":"今天利用helm去部署一个应用， 因为一些问题想删掉部署的应用。 利用helm执行了删除操作后，还有几个顽固红彤彤的pod在那挂着。 用kubectl去查询pod, 查询结果显示Error from server (NotFound): pods &quot;istio-pilot-9b8bb4dcb-k2knj&quot; not found。 呵呵，dashboard刷新n次， 那pod还在，真是扎眼。 为了真正的删除资源，只有操作etcd了。 连上服务器直接ectdctl --help,可以用etcdctl ls试试水，结果 然后百度一番执行了export ETCDCTL_API=3。 然后etcdctl get /registry/deployments/default --prefix --keys-only 然后删掉那顽固的pod,ok了 etcdctl del /registry/pods/istio-system/istio-sidecar-injector-7889c4fdf5-gnbpketcdctl del /registry/pods/istio-system/istio-pilot-db45c47d9-bzt9retcdctl del /registry/pods/istio-system/istio-sidecar-injector-645c89bc64-fh298","tags":[{"name":"etcd","slug":"etcd","permalink":"https://www.zhengyuyan.com/tags/etcd/"}]},{"title":"Hello World","date":"2018-06-27T10:31:29.696Z","path":"201806/27/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","tags":[]}]